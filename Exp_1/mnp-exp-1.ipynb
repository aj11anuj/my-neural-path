{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RF (Baseline)","metadata":{}},{"cell_type":"code","source":"import time\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load MNIST dataset\nmnist = fetch_openml('mnist_784', version=1, as_frame=False)\nX, y = mnist.data, mnist.target.astype(int)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n\n# Train the model\nstart_time = time.time()\nrf_model.fit(X_train, y_train)\ntrain_time = time.time() - start_time\n\n# Predict on test data\ny_pred = rf_model.predict(X_test)\nrf_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"Random Forest Accuracy: {rf_accuracy * 100:.2f}%\")\nprint(f\"Training Time: {train_time:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T05:26:29.024495Z","iopub.execute_input":"2025-02-17T05:26:29.024830Z","iopub.status.idle":"2025-02-17T05:27:11.605458Z","shell.execute_reply.started":"2025-02-17T05:26:29.024801Z","shell.execute_reply":"2025-02-17T05:27:11.604434Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n","output_type":"stream"},{"name":"stdout","text":"Random Forest Accuracy: 96.73%\nTraining Time: 15.17 seconds\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# RF (Optuna)","metadata":{}},{"cell_type":"code","source":"import time\nimport optuna\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load MNIST dataset\nmnist = fetch_openml('mnist_784', version=1, as_frame=False)\nX, y = mnist.data, mnist.target.astype(int)  # Ensure target labels are integers\n\n# Split dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the objective function for Optuna optimization\ndef optuna_objective(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n    max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42, n_jobs=-1)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return accuracy_score(y_test, y_pred)\n\n# Run Optuna optimization\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(optuna_objective, n_trials=20)\nbest_params_optuna = study.best_params\n\n# Train optimized RandomForest model using Optuna\nrf_model_optuna = RandomForestClassifier(n_estimators=best_params_optuna['n_estimators'], max_depth=best_params_optuna['max_depth'], random_state=42, n_jobs=-1)\nstart_time = time.time()\nrf_model_optuna.fit(X_train, y_train)\ntrain_time_optuna = time.time() - start_time\n\ny_pred_optuna = rf_model_optuna.predict(X_test)\nrf_accuracy_optuna = accuracy_score(y_test, y_pred_optuna)\n\n# Output results\nprint(f\"Optuna Optimized Random Forest Accuracy: {rf_accuracy_optuna * 100:.2f}%\")\nprint(f\"Optuna Training Time: {train_time_optuna:.2f} seconds\")\nprint(f\"Optuna Best Parameters: n_estimators={best_params_optuna['n_estimators']}, max_depth={best_params_optuna['max_depth']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T05:48:28.386331Z","iopub.execute_input":"2025-02-17T05:48:28.386665Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n[I 2025-02-17 05:48:49,469] A new study created in memory with name: no-name-f66400ba-1992-409d-a5ca-648b316d13b3\n[I 2025-02-17 05:49:06,638] Trial 0 finished with value: 0.9663571428571428 and parameters: {'n_estimators': 114, 'max_depth': 20}. Best is trial 0 with value: 0.9663571428571428.\n[I 2025-02-17 05:49:35,213] Trial 1 finished with value: 0.9680714285714286 and parameters: {'n_estimators': 186, 'max_depth': 27}. Best is trial 1 with value: 0.9680714285714286.\n[I 2025-02-17 05:49:47,077] Trial 2 finished with value: 0.9657142857142857 and parameters: {'n_estimators': 77, 'max_depth': 26}. Best is trial 1 with value: 0.9680714285714286.\n[I 2025-02-17 05:50:13,215] Trial 3 finished with value: 0.9521428571428572 and parameters: {'n_estimators': 235, 'max_depth': 11}. Best is trial 1 with value: 0.9680714285714286.\n[I 2025-02-17 05:50:41,265] Trial 4 finished with value: 0.9664285714285714 and parameters: {'n_estimators': 194, 'max_depth': 18}. Best is trial 1 with value: 0.9680714285714286.\n[I 2025-02-17 05:51:13,281] Trial 5 finished with value: 0.967 and parameters: {'n_estimators': 218, 'max_depth': 19}. Best is trial 1 with value: 0.9680714285714286.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# RF (Bald Eagle)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cupy as cp\nimport random\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.datasets import mnist\nimport time\n\n# Load and preprocess MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train.reshape(len(x_train), -1) / 255.0, x_test.reshape(len(x_test), -1) / 255.0\n\n# BEO Parameters\npopulation_size = 10\ngenerations = 5\nexplore_ratio = 0.5 \nexploit_ratio = 0.5  \n\n# Initialize population\npopulation = [{'n_estimators': random.randint(50, 150), 'max_depth': random.randint(5, 30)} for _ in range(population_size)]\n\nbest_solution, best_score = None, float('-inf')\n\nfor gen in range(generations):\n    print(f\"Generation {gen + 1}/{generations} started...\")\n    start_time = time.time()\n    scores = []\n    for idx, params in enumerate(population):\n        model = RandomForestClassifier(n_estimators=params['n_estimators'], max_depth=params['max_depth'], random_state=42, n_jobs=-1)\n        model.fit(x_train, y_train)\n        acc = accuracy_score(y_test, model.predict(x_test))\n        scores.append((params, acc))\n        print(f\"  Particle {idx + 1}: n_estimators={params['n_estimators']}, max_depth={params['max_depth']}, Accuracy={acc:.4f}\")\n        \n        if acc > best_score:\n            best_score, best_solution = acc, params\n\n    # BEO-inspired Adjustment (simple guided search)\n    avg_n_estimators = np.mean([p['n_estimators'] for p, _ in scores])\n    avg_max_depth = np.mean([p['max_depth'] for p, _ in scores])\n    population = [{'n_estimators': int(avg_n_estimators + random.uniform(-20, 20)), 'max_depth': int(avg_max_depth + random.uniform(-5, 5))} for _ in range(population_size)]\n    \n    print(f\"Generation {gen + 1} completed in {time.time() - start_time:.2f} seconds\")\n    print(f\"Current Best Score: {best_score:.4f}\")\n\nprint(f\"\\nFinal Best Hyperparameters: {best_solution}\")\nprint(f\"Final Best Accuracy: {best_score * 100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RF (Particle Swarm)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport random\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.datasets import mnist\nimport time\n\n# Load and preprocess MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train.reshape(len(x_train), -1) / 255.0, x_test.reshape(len(x_test), -1) / 255.0\n\n# PSO Parameters\nnum_particles = 10  # Number of particles in the swarm\nnum_iterations = 5  # Number of iterations (generations)\nw = 0.7  # Inertia weight\nc1 = 1.5  # Cognitive coefficient\nc2 = 1.5  # Social coefficient\n\n# Initialize particles\nparticles = [{'n_estimators': random.randint(50, 200), 'max_depth': random.randint(5, 50)} for _ in range(num_particles)]\nvelocities = [{'n_estimators': random.uniform(-10, 10), 'max_depth': random.uniform(-5, 5)} for _ in range(num_particles)]\nbest_positions = particles.copy()\nbest_scores = [float('-inf')] * num_particles\n\n# Global best\nglobal_best_position = None\nglobal_best_score = float('-inf')\n\nfor iteration in range(num_iterations):\n    print(f\"Iteration {iteration + 1}/{num_iterations} started...\")\n    start_time = time.time()\n    \n    for i, params in enumerate(particles):\n        model = RandomForestClassifier(n_estimators=params['n_estimators'], max_depth=params['max_depth'], random_state=42, n_jobs=-1)\n        model.fit(x_train, y_train)\n        acc = accuracy_score(y_test, model.predict(x_test))\n        \n        if acc > best_scores[i]:\n            best_scores[i] = acc\n            best_positions[i] = params.copy()\n        \n        if acc > global_best_score:\n            global_best_score = acc\n            global_best_position = params.copy()\n        \n        print(f\"  Particle {i + 1}: n_estimators={params['n_estimators']}, max_depth={params['max_depth']}, Accuracy={acc:.4f}\")\n    \n    # Update velocities and positions\n    for i in range(num_particles):\n        velocities[i]['n_estimators'] = (w * velocities[i]['n_estimators'] +\n                                         c1 * random.random() * (best_positions[i]['n_estimators'] - particles[i]['n_estimators']) +\n                                         c2 * random.random() * (global_best_position['n_estimators'] - particles[i]['n_estimators']))\n        velocities[i]['max_depth'] = (w * velocities[i]['max_depth'] +\n                                      c1 * random.random() * (best_positions[i]['max_depth'] - particles[i]['max_depth']) +\n                                      c2 * random.random() * (global_best_position['max_depth'] - particles[i]['max_depth']))\n        \n        particles[i]['n_estimators'] = max(10, int(particles[i]['n_estimators'] + velocities[i]['n_estimators']))\n        particles[i]['max_depth'] = max(1, int(particles[i]['max_depth'] + velocities[i]['max_depth']))\n    \n    print(f\"Iteration {iteration + 1} completed in {time.time() - start_time:.2f} seconds\")\n    print(f\"Current Best Score: {global_best_score:.4f}\")\n\nprint(f\"\\nFinal Best Hyperparameters: {global_best_position}\")\nprint(f\"Final Best Accuracy: {global_best_score * 100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SVM (Baseline)","metadata":{}},{"cell_type":"code","source":"import time\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load MNIST dataset\nmnist = fetch_openml('mnist_784', version=1, as_frame=False)\nX, y = mnist.data, mnist.target.astype(int)  # Ensure target labels are integers\n\n# Split dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train baseline SVM model\nsvm_model = SVC()\nstart_time = time.time()\nsvm_model.fit(X_train, y_train)\ntrain_time_svm = time.time() - start_time\n\ny_pred_svm = svm_model.predict(X_test)\nsvm_accuracy = accuracy_score(y_test, y_pred_svm)\n\n# Output results\nprint(f\"Baseline SVM Accuracy: {svm_accuracy * 100:.2f}%\")\nprint(f\"Baseline SVM Training Time: {train_time_svm:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T10:19:48.018306Z","iopub.execute_input":"2025-02-11T10:19:48.018701Z","iopub.status.idle":"2025-02-11T10:27:13.345693Z","shell.execute_reply.started":"2025-02-11T10:19:48.018670Z","shell.execute_reply":"2025-02-11T10:27:13.344522Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SVM (Optuna)","metadata":{}},{"cell_type":"code","source":"import time\nimport numpy as np\nimport random\nimport optuna\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load MNIST dataset\nmnist = fetch_openml('mnist_784', version=1, as_frame=False)\nX, y = mnist.data, mnist.target.astype(int)  # Ensure target labels are integers\n\n# Split dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Optuna optimization function for SVM\ndef objective(trial):\n    C = trial.suggest_loguniform('C', 1e-2, 1e2)\n    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n    \n    model = SVC(C=C, kernel=kernel)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return accuracy_score(y_test, y_pred)\n\n# Run Optuna optimization\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=20)\n\n# Get best parameters from Optuna\nbest_params_optuna = study.best_params\nbest_accuracy_optuna = study.best_value\n\n# Train SVM with Optuna optimized parameters\nsvm_model_optuna = SVC(C=best_params_optuna[\"C\"], kernel=best_params_optuna[\"kernel\"])\nstart_time = time.time()\nsvm_model_optuna.fit(X_train, y_train)\ntrain_time_optuna = time.time() - start_time\n\ny_pred_optuna = svm_model_optuna.predict(X_test)\nsvm_accuracy_optuna = accuracy_score(y_test, y_pred_optuna)\n\n# Output results\nprint(f\"Optuna Optimized SVM Accuracy: {svm_accuracy_optuna * 100:.2f}%\")\nprint(f\"Optuna Optimized SVM Training Time: {train_time_optuna:.2f} seconds\")\nprint(f\"Best Parameters Found by Optuna: C={best_params_optuna['C']}, Kernel={best_params_optuna['kernel']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T23:13:39.271961Z","iopub.execute_input":"2025-02-12T23:13:39.272299Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SVM (Bald Eagle)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport random\nfrom tensorflow.keras.datasets import mnist\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Load MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Normalize pixel values to [0, 1] range\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Flatten images for SVM\nx_train = x_train.reshape(x_train.shape[0], -1)\nx_test = x_test.reshape(x_test.shape[0], -1)\n\n# Reduce training set to 10,000 samples\nsubset_size = 10000\nx_train, y_train = x_train[:subset_size], y_train[:subset_size]\n\n# BEA-inspired Optimization for SVM\ndef bea_optimize():\n    best_hyperparams = None\n    best_acc = 0\n    no_improvement_count = 0\n    \n    population_size = 3\n    generations = 2\n    \n    # Randomly initialize population of hyperparameter sets\n    population = [{\n        'C': random.choice([0.1, 1, 10, 20]),\n        'kernel': random.choice(['linear', 'rbf']),\n        'gamma': 'scale'\n    } for _ in range(population_size)]\n    \n    for gen in range(generations):\n        print(f\"Generation {gen+1}...\")\n        prev_best_acc = best_acc\n        for params in population:\n            model = SVC(C=params['C'], kernel=params['kernel'], gamma=params['gamma'])\n            model.fit(x_train, y_train)\n            y_pred = model.predict(x_test)\n            acc = accuracy_score(y_test, y_pred)\n            \n            if acc > best_acc:\n                best_acc = acc\n                best_hyperparams = params\n        \n        if best_acc == prev_best_acc:\n            no_improvement_count += 1\n        else:\n            no_improvement_count = 0\n        \n        if no_improvement_count >= 1:\n            print(\"Early stopping triggered.\")\n            break\n    \n    print(f\"Best Hyperparameters: {best_hyperparams}\")\n    print(f\"Best Accuracy: {best_acc * 100:.2f}%\")\n    return best_hyperparams\n\n# Run BEA-inspired optimization\nbest_params = bea_optimize()\n\n# Train final SVM model with best hyperparameters\nfinal_model = SVC(C=best_params['C'], kernel=best_params['kernel'], gamma=best_params['gamma'])\nfinal_model.fit(x_train, y_train)\ny_pred = final_model.predict(x_test)\nfinal_acc = accuracy_score(y_test, y_pred)\nprint(f\"Final Optimized SVM Test Accuracy: {final_acc * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T05:59:00.001257Z","iopub.execute_input":"2025-02-13T05:59:00.001623Z","iopub.status.idle":"2025-02-13T06:04:10.714128Z","shell.execute_reply.started":"2025-02-13T05:59:00.001592Z","shell.execute_reply":"2025-02-13T06:04:10.713117Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SVM (Particle Swarm)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport random\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.datasets import mnist\nfrom sklearn.decomposition import PCA\nimport time\n\n# Load and preprocess MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train.reshape(len(x_train), -1) / 255.0, x_test.reshape(len(x_test), -1) / 255.0\n\n# Reduce dimensionality with PCA (keep 95% variance)\npca = PCA(0.95)\nx_train = pca.fit_transform(x_train)\nx_test = pca.transform(x_test)\n\n# PSO Parameters\nnum_particles = 10  # Number of particles in the swarm\nnum_iterations = 5  # Number of iterations (generations)\nw = 0.7  # Inertia weight\nc1 = 1.5  # Cognitive coefficient\nc2 = 1.5  # Social coefficient\n\n# Initialize particles\nparticles = [{'C': random.uniform(0.1, 10), 'gamma': random.choice(['scale', 'auto']), 'kernel': random.choice(['linear', 'rbf'])} for _ in range(num_particles)]\nvelocities = [{'C': random.uniform(-1, 1)} for _ in range(num_particles)]\nbest_positions = particles.copy()\nbest_scores = [float('-inf')] * num_particles\n\n# Global best\nglobal_best_position = None\nglobal_best_score = float('-inf')\n\nfor iteration in range(num_iterations):\n    print(f\"Iteration {iteration + 1}/{num_iterations} started...\")\n    start_time = time.time()\n    \n    for i, params in enumerate(particles):\n        model = SVC(C=params['C'], kernel=params['kernel'], gamma=params['gamma'])\n        model.fit(x_train, y_train)\n        acc = accuracy_score(y_test, model.predict(x_test))\n        \n        if acc > best_scores[i]:\n            best_scores[i] = acc\n            best_positions[i] = params.copy()\n        \n        if acc > global_best_score:\n            global_best_score = acc\n            global_best_position = params.copy()\n        \n        print(f\"  Particle {i + 1}: C={params['C']}, Kernel={params['kernel']}, Gamma={params['gamma']}, Accuracy={acc:.4f}\")\n    \n    # Update velocities and positions\n    for i in range(num_particles):\n        velocities[i]['C'] = (w * velocities[i]['C'] +\n                              c1 * random.random() * (best_positions[i]['C'] - particles[i]['C']) +\n                              c2 * random.random() * (global_best_position['C'] - particles[i]['C']))\n        \n        particles[i]['C'] = max(0.1, particles[i]['C'] + velocities[i]['C'])\n    \n    print(f\"Iteration {iteration + 1} completed in {time.time() - start_time:.2f} seconds\")\n    print(f\"Current Best Score: {global_best_score:.4f}\")\n\nprint(f\"\\nFinal Best Hyperparameters: {global_best_position}\")\nprint(f\"Final Best Accuracy: {global_best_score * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:46:22.448096Z","iopub.execute_input":"2025-02-17T08:46:22.448548Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nIteration 1/5 started...\n  Particle 1: C=5.149958702059902, Kernel=linear, Gamma=scale, Accuracy=0.9430\n  Particle 2: C=1.2941433556778874, Kernel=rbf, Gamma=scale, Accuracy=0.9840\n  Particle 3: C=6.439252660962551, Kernel=linear, Gamma=auto, Accuracy=0.9417\n  Particle 4: C=3.6046663238581695, Kernel=rbf, Gamma=auto, Accuracy=0.9794\n  Particle 5: C=0.43169650674226934, Kernel=linear, Gamma=scale, Accuracy=0.9457\n  Particle 6: C=7.299701051771945, Kernel=linear, Gamma=auto, Accuracy=0.9420\n  Particle 7: C=2.1176633082563137, Kernel=linear, Gamma=scale, Accuracy=0.9446\n  Particle 8: C=5.325918895883778, Kernel=rbf, Gamma=auto, Accuracy=0.9802\n  Particle 9: C=3.1578531346331795, Kernel=linear, Gamma=scale, Accuracy=0.9439\n  Particle 10: C=8.2332568509797, Kernel=rbf, Gamma=auto, Accuracy=0.9819\nIteration 1 completed in 1816.67 seconds\nCurrent Best Score: 0.9840\nIteration 2/5 started...\n  Particle 1: C=1.205413267273821, Kernel=linear, Gamma=scale, Accuracy=0.9447\n  Particle 2: C=1.9587090277269468, Kernel=rbf, Gamma=scale, Accuracy=0.9845\n  Particle 3: C=4.4658642668545605, Kernel=linear, Gamma=auto, Accuracy=0.9428\n  Particle 4: C=1.8496086644072236, Kernel=rbf, Gamma=auto, Accuracy=0.9759\n  Particle 5: C=0.831975429340569, Kernel=linear, Gamma=scale, Accuracy=0.9442\n  Particle 6: C=5.299589987896969, Kernel=linear, Gamma=auto, Accuracy=0.9431\n  Particle 7: C=1.309393278410399, Kernel=linear, Gamma=scale, Accuracy=0.9444\n  Particle 8: C=2.9324308920303306, Kernel=rbf, Gamma=auto, Accuracy=0.9780\n  Particle 9: C=3.186097903028986, Kernel=linear, Gamma=scale, Accuracy=0.9436\n  Particle 10: C=5.597758748659563, Kernel=rbf, Gamma=auto, Accuracy=0.9803\nIteration 2 completed in 1526.22 seconds\nCurrent Best Score: 0.9845\nIteration 3/5 started...\n  Particle 1: C=0.1, Kernel=linear, Gamma=scale, Accuracy=0.9457\n  Particle 2: C=2.423904998161288, Kernel=rbf, Gamma=scale, Accuracy=0.9847\n  Particle 3: C=2.4129948231831007, Kernel=linear, Gamma=auto, Accuracy=0.9443\n  Particle 4: C=3.1453304408431193, Kernel=rbf, Gamma=auto, Accuracy=0.9787\n  Particle 5: C=1.0329614018469901, Kernel=linear, Gamma=scale, Accuracy=0.9448\n  Particle 6: C=3.466790503695945, Kernel=linear, Gamma=auto, Accuracy=0.9436\n  Particle 7: C=2.1390948563081067, Kernel=linear, Gamma=scale, Accuracy=0.9446\n  Particle 8: C=0.9768032301092175, Kernel=rbf, Gamma=auto, Accuracy=0.9717\n  Particle 9: C=2.6939149873335717, Kernel=linear, Gamma=scale, Accuracy=0.9443\n  Particle 10: C=5.3849147021548855, Kernel=rbf, Gamma=auto, Accuracy=0.9802\nIteration 3 completed in 1319.84 seconds\nCurrent Best Score: 0.9847\nIteration 4/5 started...\n  Particle 1: C=1.211027049832253, Kernel=linear, Gamma=scale, Accuracy=0.9446\n  Particle 2: C=2.7495421774653273, Kernel=rbf, Gamma=scale, Accuracy=0.9850\n  Particle 3: C=0.9888268997607981, Kernel=linear, Gamma=auto, Accuracy=0.9448\n  Particle 4: C=4.23668737276301, Kernel=rbf, Gamma=auto, Accuracy=0.9792\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## 3 layered CNN","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\nimport numpy as np\n\n# Load MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Normalize pixel values to [0, 1] range\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Expand dimensions to match CNN input shape\nx_train = np.expand_dims(x_train, axis=-1)\nx_test = np.expand_dims(x_test, axis=-1)\n\n# Define CNN model\nmodel = keras.Sequential([\n    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n    layers.MaxPooling2D(pool_size=(2, 2)),\n    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n    layers.MaxPooling2D(pool_size=(2, 2)),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\n\n# Compile model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train model\nmodel.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n\n# Evaluate model\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(f\"Test Accuracy: {test_acc * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:26:54.605161Z","iopub.execute_input":"2025-02-17T08:26:54.605515Z","iopub.status.idle":"2025-02-17T08:32:39.602851Z","shell.execute_reply.started":"2025-02-17T08:26:54.605482Z","shell.execute_reply":"2025-02-17T08:32:39.601843Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 17ms/step - accuracy: 0.9190 - loss: 0.2925 - val_accuracy: 0.9876 - val_loss: 0.0367\nEpoch 2/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 16ms/step - accuracy: 0.9857 - loss: 0.0436 - val_accuracy: 0.9843 - val_loss: 0.0492\nEpoch 3/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 16ms/step - accuracy: 0.9903 - loss: 0.0280 - val_accuracy: 0.9864 - val_loss: 0.0403\nEpoch 4/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 16ms/step - accuracy: 0.9943 - loss: 0.0175 - val_accuracy: 0.9924 - val_loss: 0.0231\nEpoch 5/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 16ms/step - accuracy: 0.9952 - loss: 0.0139 - val_accuracy: 0.9912 - val_loss: 0.0242\nEpoch 6/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 16ms/step - accuracy: 0.9961 - loss: 0.0105 - val_accuracy: 0.9911 - val_loss: 0.0279\nEpoch 7/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 17ms/step - accuracy: 0.9973 - loss: 0.0091 - val_accuracy: 0.9894 - val_loss: 0.0362\nEpoch 8/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 16ms/step - accuracy: 0.9970 - loss: 0.0083 - val_accuracy: 0.9890 - val_loss: 0.0397\nEpoch 9/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 16ms/step - accuracy: 0.9976 - loss: 0.0068 - val_accuracy: 0.9918 - val_loss: 0.0350\nEpoch 10/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.9984 - loss: 0.0045 - val_accuracy: 0.9914 - val_loss: 0.0377\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9898 - loss: 0.0446\nTest Accuracy: 99.14%\n","output_type":"stream"}],"execution_count":1}]}